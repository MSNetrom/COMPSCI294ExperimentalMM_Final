{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "from utils import prepare_data_loaders, unpack_dataset\n",
    "from main import evaluate_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Amount of data in the dataset: 983040000 bits\n",
      "Memory Equivalent Capacity of table as lookup-table: 132877.1237954945 bits\n",
      "Maximum mutual information between labels and data: 132875.3853 bits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating joint entropies: 100%|██████████| 3072/3072 [00:32<00:00, 95.22it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum mutual information between labels and data: 9687.2715 bits\n",
      "Meaning we need to memorize 123188.1172 bits of information to predict the labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cifar_data_loaders, cifar_data_sets, transform = prepare_data_loaders()\n",
    "\n",
    "data, labels = unpack_dataset(cifar_data_sets[\"train_data\"])\n",
    "# Do some evaluation on the data\n",
    "result_dict = evaluate_data(data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MEC\n",
    "\n",
    "The MEC of the first layer of a MLP is given as\n",
    "\n",
    "$$\n",
    "MEC = nodes \\cdot parameters/node = hiddensize \\cdot (inputsize + 1)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $inputsize = hiddensize$ and we get:\n",
    "\n",
    "$$\n",
    "MEC = hiddensize \\cdot (hiddensize + 1) = hiddensize^2 + hiddensize\n",
    "$$\n",
    "$$\n",
    "hiddensize = \\frac{-1 + \\sqrt{1 + 4 \\cdot MEC}}{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Equivalent Capacity: 132877.1237954945\n",
      "Hidden states neede: 364.02348867459074\n",
      "Input size original: 3072\n",
      "Reduction factor: 0.11849722938626 / 8.439015875555585\n"
     ]
    }
   ],
   "source": [
    "mem_cap = result_dict[\"memory_equivalent_capacity\"]\n",
    "hidden_states = (-1 + math.sqrt(1 + 4 * mem_cap)) / 2\n",
    "input_size_original = 3072\n",
    "\n",
    "print(f\"Memory Equivalent Capacity: {mem_cap}\")\n",
    "print(f\"Hidden states neede: {hidden_states}\")\n",
    "print(f\"Input size original: {input_size_original}\")\n",
    "print(f\"Reduction factor: {hidden_states / input_size_original} / {input_size_original / hidden_states}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see. The data can be reduced by almost a factor of 10 by a CNN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
